# -*- coding: utf-8 -*-
"""ADBE_model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1y-Tnv9EPEA4gtS7MIUDwOH-l1-uM-AU6
"""

import json
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from app.config import TOP_COMPANIES
from torch.utils.data import DataLoader
from predict.ml_utils import utils
config = {
    "data": {
        "window_size": 20,
        "train_split_size": 0.80,
    },
    "model": {
        "input_size": 1,  # since we are only using 1 feature, close price
        "num_lstm_layers": 2,
        "lstm_size": 32,
        "dropout": 0.2,
    },
    "training": {
        "device": "cpu",  # "cuda" or "cpu"
        "batch_size": 64,
        "num_epoch": 100,
        "learning_rate": 0.01,
        "scheduler_step_size": 60,
    },
}


def run_epoch(dataloader, is_training=False):
    epoch_loss = 0

    if is_training:
        model.train()
    else:
        model.eval()

    for idx, (x, y) in enumerate(dataloader):
        if is_training:
            optimizer.zero_grad()

        batchsize = x.shape[0]

        x = x.to(config["training"]["device"])
        y = y.to(config["training"]["device"])

        out = model(x)
        loss = criterion(out.contiguous(), y.contiguous())

        if is_training:
            loss.backward()
            optimizer.step()

        epoch_loss += loss.detach().item() / batchsize

    lr = scheduler.get_last_lr()[0]

    return epoch_loss, lr
random_seed = 7
torch.manual_seed(random_seed)
results = {}
for symbol in TOP_COMPANIES:

    """Import data from Yfinance"""
    df = utils.fetch_time_series(symbol)
    data_close = np.array(df["Close"])

    """normalize data to define standard dev and mean"""

    scaler = utils.Normalizer()
    normalized_data_close_price = scaler.fit_transform(data_close)
    """dividng data in training, val and test"""
    data_x, data_x_unseen = utils.prepare_data_x(
        normalized_data_close_price, window_size=config["data"]["window_size"]
    )
    data_y = utils.prepare_data_y(
        normalized_data_close_price, window_size=config["data"]["window_size"]
    )

    split_index = int(data_y.shape[0] * config["data"]["train_split_size"])
    data_x_train = data_x[:split_index]
    data_x_val = data_x[split_index:]
    data_y_train = data_y[:split_index]
    data_y_val = data_y[split_index:]


    dataset_train = utils.TimeSeriesDataset(data_x_train, data_y_train)
    dataset_val = utils.TimeSeriesDataset(data_x_val, data_y_val)

    print("Train data shape", dataset_train.x.shape, dataset_train.y.shape)
    print("Validation data shape", dataset_val.x.shape, dataset_val.y.shape)

    train_dataloader = DataLoader(
        dataset_train, batch_size=config["training"]["batch_size"], shuffle=True
    )
    val_dataloader = DataLoader(
        dataset_val, batch_size=config["training"]["batch_size"], shuffle=True
    )

    model = utils.LSTMModel(
        input_size=config["model"]["input_size"],
        hidden_layer_size=config["model"]["lstm_size"],
        num_layers=config["model"]["num_lstm_layers"],
        output_size=1,
        dropout=config["model"]["dropout"],
    )

    model = model.to(config["training"]["device"])

    criterion = nn.MSELoss()
    optimizer = optim.Adam(
        model.parameters(),
        lr=config["training"]["learning_rate"],
        betas=(0.9, 0.98),
        eps=1e-9,
    )
    scheduler = optim.lr_scheduler.StepLR(
        optimizer, step_size=config["training"]["scheduler_step_size"], gamma=0.1
    )

    for epoch in range(config["training"]["num_epoch"]):
        loss_train, lr_train = run_epoch(train_dataloader,is_training=True)
        loss_val, lr_val = run_epoch(val_dataloader)
        scheduler.step()

        print(
            "Epoch[{}/{}] | loss train:{:.6f}, test:{:.6f} | lr:{:.6f}".format(
                epoch + 1, config["training"]["num_epoch"], loss_train, loss_val, lr_train
            )
        )



    """ Model evaluation """

    train_dataloader = DataLoader(
        dataset_train, batch_size=config["training"]["batch_size"], shuffle=False
    )
    val_dataloader = DataLoader(
        dataset_val, batch_size=config["training"]["batch_size"], shuffle=False
    )
    
    model.eval()

# predict on the training data, to see how well the model managed to learn and memorize

    predicted_train = np.array([])

    for idx, (x, y) in enumerate(train_dataloader):
        x = x.to(config["training"]["device"])
        out = model(x)
        out = out.cpu().detach().numpy()
        predicted_train = np.concatenate((predicted_train, out))
    # predict on the validation data, to see how the model does

    predicted_val = np.array([])

    for idx, (x, y) in enumerate(val_dataloader):
        x = x.to(config["training"]["device"])
        out = model(x)
        out = out.cpu().detach().numpy()
        predicted_val = np.concatenate((predicted_val, out))

    limit = 292
    data_y_test = np.zeros(limit)
    data_y_val_pred = np.zeros(limit)
    data_y_test_pred = np.zeros(limit)
    data_y_test = scaler.inverse_transform(data_y_val)
    data_y_val_pred= scaler.inverse_transform(predicted_val)

   

    y_true = data_y_test.tolist()
    y_pred = data_y_val_pred.tolist()
    true_res = [x - y_true[i - 1] for i, x in enumerate(y_true)]
    true_results = []
    
    for i in true_res:
        if i > 0:
            true_results.append("Positive")
        else:
            true_results.append("Negative")

    pred_res = [x - y_pred[i - 1] for i, x in enumerate(y_pred)]
    pred_results = []
    for i in pred_res:
        if i > 0:
            pred_results.append("Positive")
        else:
            pred_results.append("Negative")

    accuracy = utils.accuracy_score(true_results, pred_results)
    print(accuracy)
    results[symbol] = accuracy

    torch.save(model, f=f"predict/models/{symbol}.pt")


with open("predict/accuracy.json", "w+", encoding="utf-8") as f:
    json.dump(results, f, ensure_ascii=False, indent=4)
